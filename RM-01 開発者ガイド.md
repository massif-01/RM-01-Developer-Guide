# RM-01 開発者ガイド

**使用前に必ずお読みください：**

RM-01は、**推論モジュール**、**アプリケーション・モジュール**、および**暗号化・管理チップ**（以下、管理モジュールと呼ぶ）で構成されており、これらは**オンボード・イーサネット・スイッチチップ**を介して相互接続され、内部LANサブネットを形成します。ユーザーが**USB Type-C**インターフェースを介してホスト（例：PC、スマートフォン、iPad）に接続すると、RM-01はUSBイーサネット機能を通じてホストに仮想イーサネットインターフェースを提供します。ホストはIPアドレスを取得し、自動的にサブネットに参加してデータ交換を行います。

デバイスが電源を入れて**USB Type-C**インターフェースを介してホストに接続されると、システムはローカルネットワークサブネットを自動的に構成します。**ユーザー・ホスト**には静的IPアドレス`10.10.99.100`が割り当てられます。**推論モジュール**（IP：`10.10.99.98`）および**アプリケーション・モジュール**（IP：`10.10.99.99`）は、それぞれ独立したSSHサービスを展開しており、ユーザーは標準的なSSHクライアント（例：OpenSSH、PuTTY）を介して直接アクセスできます。一方、管理モジュールはシリアルポートツールを介してアクセスする必要があります。

---

## ホストからRM-01にインターネットアクセスを提供する方法（macOSを例に）

**ユーザー・ホスト**をUSB Type-Cで接続した後、RM-01はネットワークインターフェースリストに以下のように表示されます：  
- **`AX88179A`**（開発者向けバージョン）  
- **`RMinte RM-01`**（商用リリースバージョン）

インターネット共有を設定するには、以下の手順に従ってください：

1. **システム設定**（System Settings）を開きます  
2. **ネットワーク**（Network）→**共有**（Sharing）に進みます  
3. **インターネット共有**（Internet Sharing）を有効にします  
4. 共有設定の横にある**「i」アイコン**をクリックして、設定画面に入ります：  
   - **「以下のソースから接続を共有」**（Share your connection from）を**Wi-Fi**に設定します  
   - **「以下のポートを使用してコンピュータに共有」**（To computers using）で、**AX88179A**または**RMinte RM-01**（デバイスのモデルに応じて選択）をチェックします  
5. **完了**（Done）をクリックします

その後、**ネットワーク**（Network）設定ページに戻り、RM-01のネットワークインターフェースを手動で設定します：  
- **IPアドレス**：`10.10.99.100`  
- **サブネットマスク**：`255.255.255.0`  
- **ルーター**（Router）：`10.10.99.100`（ホスト自身のIP）

> **注**：  
> この設定により、ホストがゲートウェイとして機能し、RM-01にNATネットワークアクセスを提供します。RM-01のデフォルトゲートウェイおよびDNSは、ホストのDHCPサービスによって自動的に割り当てられます。IPを手動で設定することで、デバイスが常に`10.10.99.0/24`サブネット内に留まり、デバイス内部のサービス通信と一貫性を保ちます。

---

## **CFexpress Type-B**ストレージカードについて

**CFexpress Type-B**ストレージカードは、RM-01デバイスの主要コンポーネントの一つであり、システム起動、推論フレームワークの展開、およびISV/SVソフトウェアの配布と認証認可の重要な機能を担います。

このストレージカードは、以下の3つの独立したパーティションに分割されています：

`rm01sys` `rm01app` `rm01models`

`rm01sys` — システムパーティション  
**推論モジュール**のオペレーティングシステムおよびコア実行環境がこのパーティションにインストールされています。ユーザーまたは開発者がこのパーティションの内容にアクセス、変更、または削除することは厳禁です。許可されていない変更は、推論モジュールが起動しない、または推論機能が動作しない原因となり、これによるハードウェアまたはソフトウェアの損傷は保証サービスの対象外です。

`rm01app` — アプリケーションパーティション  
このパーティションは、ユーザーまたは開発者が提出した`Docker`イメージファイルを一時的に保存するために使用されます。イメージが`rm01app`に書き込まれた後、RM-01システムはそれをホストに内蔵された**NVMe SSD**ストレージに自動的に移行し、コンテナ化された展開を完了します。このパーティション内でアプリケーションを直接実行または変更しないでください。

`rm01models` — モデルパーティション  
このパーティションは、ユーザーまたは開発者がロードする大規模なAIモデル（例：LLM、マルチモーダルモデルなど）を保存するために専用です。モデル形式、サイズ制限、ロード手順、および互換性要件については、以下の「モデルについて」セクションを参照してください。

---

## アプリケーション・モジュールについて

IPアドレス：`10.10.99.99`  
ポート範囲：`59000-59299`

アプリケーション・モジュールのハードウェア仕様：

```
プロセッサ：Intel Core i3-N305（8コア8スレッド、ベース周波数1.8 GHz、最大ターボ周波数3.8 GHz）  
メモリ：16 GB / 24 GB LPDDR5-4800MT/s（オンボード、拡張不可）  
ストレージ：512 GB / 1 TB / 2 TB（オプション）NVMe SSD  
```

アプリケーション・モジュールのSSHアクセス認証情報：

```
デフォルトユーザー名：rm01  
デフォルトパスワード：rm01（工場出荷時設定、初回ログイン専用）  
```

**セキュリティに関する注意：**  
システムの安全性を確保するため、初回のSSHログイン後に直ちに`passwd`コマンドを使用してデフォルトパスワードを変更してください。デフォルトパスワードは初期設定専用であり、プロダクションおよびデプロイメント環境での使用は厳禁です。

---

## 推論モジュールについて

IPアドレス：`10.10.99.98`  
サービスポート範囲：`58000–58999`

**推論モジュール**はRM-01のコア計算ユニットであり、さまざまな高性能AI推論構成をサポートします。ユーザーはモデル規模と性能要件に基づいて適切なモデルを選択できます。工場出荷時にプリセットされた4つのハードウェア構成は以下の通りです：

| メモリ | メモリ帯域幅 | 計算能力             | Tensor Core数 |
|--------|--------------|:--------------------|:--------------|
| 32 GB  | 204.8 GB/s   | 200 TOPS (INT8)     | 56            |
| 64 GB  | 204.8 GB/s   | 275 TOPS (INT8)     | 64            |
| 64 GB  | 273 GB/s     | 1,200 TFLOPS (FP4)  | 64            |
| 128 GB | 273 GB/s     | 2,070 TFLOPS (FP4)  | 96            |

RM-01は、**CFexpress Type-B**ストレージカードに以下の2つの推論フレームワークがプリインストールされており、どちらも推論モジュール上で実行されます：

`vLLM`：自動的に起動し、デフォルトでポート**58000**をリッスンし、OpenAI互換APIインターフェースを提供し、POST /v1/chat/completionsなどの標準リクエストをサポートします。  
`TEI`（Text Embedding Inference）：手動で起動する必要があり、テキスト埋め込みサービスに使用されます。

APIアクセス方法：  
モデルが正常にロードされた後、**vLLM**推論サービスは以下のアドレスを介してアクセスできます：  
`http://10.10.99.98:58000/v1/chat/completions`  
標準のOpenAIクライアント（例：openai-python、curl、Postman）を使用して直接呼び出しが可能です。

**セキュリティに関する注意：**  
システムの安全性と安定性を確保するため、推論モジュールはSSHアクセス権限を提供しません。ユーザーおよび開発者は、このモジュールの基盤となるオペレーティングシステムに直接ログインしたり、対話的に操作したりすることはできません。セキュリティポリシーを回避したり、推論モジュールに直接アクセスしようとする試みは、システムの異常、データの破損、またはサービスの停止を引き起こす可能性があり、これらは保証サービスの対象外です。

---

## モデルについて

RM-01は、以下を含むさまざまなAIモデルの推論をサポートしますが、これに限定されません：**大規模言語モデル（LLM）**、**マルチモーダルモデル（MLM）**、**視覚言語モデル（VLM）**、**テキスト埋め込みモデル（Embedding）**、および**リランカーモデル（Reranker）**。すべてのモデルファイルは、デバイスの内蔵**CFexpress Type-B**ストレージカードに保存する必要があります。ユーザーは、互換性のある**CFexpress Type-B**カードリーダーを使用して、ホスト側でモデルのアップロード、管理、更新を行う必要があります。

**CFexpress Type-B**ストレージカードがRM-01に接続されると、システムはそれを`models`という名前の読み取り専用データボリュームとしてマウントし、パスは`/home/rm01/models`です。その標準ファイル構造は以下の通りです：

```bash
models/
├── auto/                  # 自動モデルロード用ディレクトリ（プロダクション向けデプロイ）
│   ├── embedding/         # 埋め込みモデル（自動ロード非対応、以下参照）
│   ├── llm/               # 大規模言語モデル（ウェイトファイル直接格納、以下参照）
│   └── reranker/          # リランカーモデル（自動ロード非対応、以下参照）
└── dev/                   # 開発者定義モデルディレクトリ（優先度高）
    ├── embedding/
    ├── llm/
    └── reranker/
```

> **注**：  
> - `auto/`ディレクトリは**軽量かつ標準化されたモデルデプロイ**に使用され、システムによって自動的に認識されます。  
> - `dev/`ディレクトリは**開発者によるモデルの詳細な制御**に使用され、`auto/`よりも優先度が高く、システムは`auto/`内のモデルを無視します。

---

### 1. 自動モード（auto）

#### 使用方法：  
モデルの**完全なウェイトファイル**（例：`.safetensors`、`.bin`、`.pt`、`.awq`など）を**`auto/llm/`ディレクトリに直接配置**し、**サブフォルダにネストしない**でください。

> 誤った例：  
> `auto/llm/Qwen3-30B-A3B-Instruct-2507-AWQ/model.safetensors`  
>  
> 正しい例：  
> `auto/llm/model-001-of-006.safetensors`  
> `auto/llm/config.json`  
> `auto/llm/tokenizer.json`  
> `auto/llm/vocab.json`

#### システムの動作：  
- デバイス起動時に、システムは`auto/llm/`ディレクトリをスキャンし、互換性のある形式のモデルを自動的にロードします。  
- **埋め込みモデル（embedding）およびリランカーモデル（reranker）の自動ロードはサポートされていません**。LLMのみ対応。  
- モデルロード後、**デフォルトで基本的な推論機能が有効**になり、以下の**高度な機能は有効になりません**：  
  - 投機的デコーディング（Speculative Decoding）  
  - プレフィックスキャッシング（Prefix Caching）  
  - チャンク化プリフィル（Chunked Prefill）  
  - 最大コンテキスト長（`max_model_len`）はシステムの安全閾値（通常≤8192トークン）に制限されます。  
- **パフォーマンス最適化の制限**：システムの安定性とマルチタスク同時実行性を確保するため、自動モードのモデルは保守的なメモリ割り当て戦略（`gpu_memory_utilization` ≤ 0.8）を使用します。

> **重要な注意**：  
> 自動モードは**モデルの互換性を迅速に検証**または**標準化されたデプロイシナリオ**に適していますが、**プロダクション向けの高性能推論には適していません**。完全なパフォーマンスを得るには、**手動モード（dev）**を使用してください。

---

### 2. 手動モード（dev）— 開発者モード

> **手動モードは自動モードよりも優先度が高い**。`dev/`の`embedding`、`llm`、または`reranker`ディレクトリに任意の`.yaml`設定ファイルが存在する場合、システムは**`auto/`内のすべてのモデルを完全に無視**します。

#### 設定ファイルの構造：  
`dev/`の`embedding`、`llm`、および`reranker`ディレクトリには、対応するモデルサービスを開始するために、以下の3つのYAML設定ファイルを配置する必要があります：

| ファイル名            | 用途                              |
|-----------------------|-----------------------------------|
| `embedding_run.yaml`  | テキスト埋め込みモデルサービス開始 |
| `llm_run.yaml`        | 大規模言語モデルサービス開始       |
| `reranker_run.yaml`   | リランカーモデルサービス開始      |

> すべてのファイルは`dev/`内の対応するフォルダに配置する必要があり、**ファイル名は完全に一致**し、大文字小文字を区別します。

#### 設定ファイル例（`llm_run.yaml`）：

```yaml
model: /home/rm01/models/dev/llm/Qwen3-30B-A3B-Instruct-2507-AWQ
port: 58000
gpu_memory_utilization: 0.85
max_model_len: 24576
served_model_name: RM-01 LLM
enable_prefix_caching: true
enable_chunked_prefill: true
max_num_batched_tokens: 512
block_size: 16
tensor_parallel_size: 1
dtype: auto
```

> **注**：  
> - `model`パスは**絶対パス**で、モデルファイルのディレクトリ（圧縮ファイルではない）を指す必要があります。  
> - `port`は`58000–58999`の範囲内で、他のサービスと競合しないようにする必要があります。  
> - `gpu_memory_utilization`は、スループットを最大化するために0.7–0.9に設定することをお勧めします。  
> - すべてのパラメータは**vLLM公式ドキュメント** `https://docs.vllm.ai/en/latest/index.html`に準拠します。**CFexpress Type-B**ストレージカードで現在使用されているvLLMのバージョンと互換性があることを確認してください。

#### 起動プロセス：  
1. モデルファイル（完全なディレクトリ）を`dev/llm/`（または`embedding/`、`reranker/`）にコピーします。  
2. 対応する`.yaml`設定ファイルを作成して配置します。  
3. **CFexpress Type-B**ストレージカードを挿入し、RM-01を再起動します。  
4. システムは`dev/`内の設定を自動的にロードし、対応するサービスを開始します。  
5. サービス開始後、`http://10.10.99.98:58000/v1/chat/completions`などのインターフェースを介してアクセスできます。

> **推奨**：初回デプロイ時には、vLLMが提供する`--load-format auto`および`--dtype auto`オプションを使用して、モデル形式に自動的に適応することをお勧めします。

---

### セキュリティおよびメンテナンスに関する注意

- **推論モジュールへの直接SSHログインの禁止**：すべてのモデル管理は**CFexpress Type-B**ストレージカードを介して行う必要があります。  
- **モデルファイルは生のウェイトである必要がある**：圧縮ファイル（.zip/.tar.gz）、暗号化パッケージ、または非標準形式は使用しないでください。  
- **ファイルパーミッション**：すべてのモデルファイルは読み取り可能（`chmod 644`）、ディレクトリは実行可能（`chmod 755`）である必要があります。  
- **バージョン管理**：Gitまたはファイル命名規則（例：`Qwen3-30B-A3B-Instruct-v1.2-20250930`）を使用してモデルバージョンを管理することをお勧めします。  
- **バックアップ推奨**：モデルを更新する前に、`dev/`および`auto/`ディレクトリをバックアップして、設定の損失を防ぎます。

---

### モード選択の推奨

| シナリオ                        | 推奨モード                     |
|--------------------------------|-------------------------------|
| モデルの互換性を迅速に検証      | 自動モード（auto）            |
| プロダクション環境での高性能推論 | 手動モード（dev）+ 詳細設定   |
| 複数モデルの並行デプロイ       | 手動モード（dev）+ 複数の`.yaml`ファイル |
| 開発デバッグ、プロトタイプ検証  | 手動モード（dev）            |
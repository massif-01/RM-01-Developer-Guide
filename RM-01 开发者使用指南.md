# RM-01 开发者使用指南

**使用前必读：** 

RM-01 由一个 **推理模组** 、一个 **应用模组** 和一颗 **加密与管理芯片**（以下简称管理模组） 构成，三者通过 **板载以太网交换芯片** 互联，形成内部局域网子网。当用户通过 **USB Type-C** 接口连接主机（如 PC、手机、iPad）时，RM-01 会通过 USB Ethernet 功能为主机虚拟出一个以太网接口，主机随即获得 IP 地址并自动加入该子网实现数据交互。

设备上电启动并经 **USB Type-C** 接口连接主机后，系统将自动配置本地网络子网，**用户主机** 将被分配静态 IP 地址 `10.10.99.100` 。**推理模组** （IP: `10.10.99.98` ）和**应用模组** （IP: `10.10.99.99` ）——均部署独立的 SSH 服务，支持用户通过标准 SSH 客户端（如 OpenSSH、PuTTY）直接访问，而管理模组则需通过串口工具访问。

---

## 如何通过主机为 RM-01 提供互联网访问（以 macOS 为例）

在通过 USB Type-C 连接 **用户主机** 后，RM-01 将在网络接口列表中显示为：  
- **`AX88179A`**（开发者版本）  
- **`RMinte RM-01`**（商业发行版本）

请按以下步骤配置互联网共享：

1. 打开 **系统设置**（System Settings）  
2. 进入 **网络**（Network） → **共享**（Sharing）  
3. 启用 **互联网共享**（Internet Sharing）  
4. 点击共享设置旁的 **“i”图标**，进入配置界面：  
   - 将 **“共享以下来源的连接”**（Share your connection from）设置为：**Wi-Fi**  
   - 在 **“使用以下端口共享给设备”**（To computers using）中，勾选：**AX88179A** 或 **RMinte RM-01**（根据设备型号选择）  
5. 点击 **完成**（Done）

随后，返回 **网络**（Network）设置页面，手动配置 RM-01 的网络接口：  
- **IP 地址**：`10.10.99.100`  
- **子网掩码**：`255.255.255.0`  
- **路由器**（Router）：`10.10.99.100`（即主机自身 IP）

> **说明**：  
> 此配置将主机作为网关，为 RM-01 提供 NAT 网络访问。RM-01 的默认网关和 DNS 均由主机通过 DHCP 服务自动分配，手动设置 IP 可确保其始终位于 `10.10.99.0/24` 子网内，与设备内部服务通信一致。

---

## 关于 **CFexpress Type-B** 存储卡

**CFexpress Type-B** 存储卡是 RM-01 设备的核心组件之一，承担系统引导、模型推理框架部署及 ISV/SV 软件分发与授权认证的关键功能。

该存储卡划分为三个独立分区，分别为：

`rm01roofs` `rm01app` `rm01models`

`rm01sys` —— 系统分区
**推理模组** 的操作系统与核心运行环境安装于该分区。严禁用户或开发者访问、修改或删除该分区内容。任何未经授权的更改均可能导致推理模组无法启动或推理功能失效，且由此造成的硬件或软件损坏不在任何保修服务范围内。

`rm01app` —— 应用分区
该分区用于暂存用户或开发者提交的 `Docker` 镜像文件。当镜像被写入 `rm01app` 后，RM-01 系统将自动将其迁移至主机内置的 **NVMe SSD** 存储中，并完成容器化部署。请勿直接在该分区中运行或修改应用文件。

`rm01models` —— 模型分区
该分区专用于存储用户或开发者加载的大规模人工智能模型（如 LLM、多模态模型等）。关于模型格式、大小限制、加载流程及兼容性要求，请参阅下文 “关于模型” 章节。

---

## 关于应用模组

IP地址: `10.10.99.99`
端口范围: `59000-59299`

应用模组硬件规格：

```
处理器：Intel Core i3-N305（8 核 8 线程，基础频率 1.8 GHz，最大睿频 3.8 GHz）
内存：16 GB / 24 GB LPDDR5-4800MT/s（板载，不可扩展）
存储：512 GB / 1 TB / 2 TB (可选) NVMe SSD
```

应用模组 SSH 访问凭证：

```
默认用户名：rm01
默认密码：rm01（出厂预设，仅用于首次登录）
```

**安全须知：**
为保障系统安全，首次通过 SSH 登录后，请立即使用 `passwd` 命令修改默认密码；默认密码仅用于初始配置，严禁在生产及交付环境中使用。

---

## 关于推理模组

IP 地址：`10.10.99.98`
服务端口范围：`58000–58999`

**推理模组** 是 RM-01 的核心计算单元，支持多种高性能 AI 推理配置，用户可根据模型规模与性能需求选择对应型号。出厂预置的四种硬件配置如下：

| 显存   | 显存带宽   | 算力                | Tensor Core 数量 |
|--------|------------|:--------------------|:-----------------|
| 32 GB  | 204.8 GB/s | 200 TOPS  (INT8)    | 56               |
| 64 GB  | 204.8 GB/s | 275 TOPS  (INT8)    | 64               |
| 64 GB  | 273 GB/s   | 1,200 TFLOPS  (FP4) | 64               |
| 128 GB | 273 GB/s   | 2,070 TFLOPS  (FP4) | 96               |


RM-01 出厂时，**CFexpress Type-B** 存储卡中预装以下两个推理框架，均运行于推理模组：

`vLLM`：自动启动，默认监听端口 **58000**，提供 OpenAI 兼容 API 接口，支持标准 POST /v1/chat/completions 等请求。
`TEI` (Text Embedding Inference)：需手动启动，用于文本嵌入（Embedding）服务。

API 访问方式：
成功加载模型后，可通过以下地址访问 **vLLM** 推理服务：
`http://10.10.99.98:58000/v1/chat/completions`
支持标准 OpenAI 客户端（如 openai-python、curl、Postman）直接调用。

**安全须知：**
为保障系统安全与稳定性，推理模组不开放 SSH 访问权限，用户与开发者无法通过任何方式直接登录或交互式操作该模块的底层操作系统。
任何试图绕过安全策略、直接访问推理模组的操作，均可能导致系统异常、数据损坏或服务中断，且不在保修服务范围内。

---

## 关于模型

RM-01 支持推理多种人工智能模型，包括但不限于：**大语言模型（LLM）**、**多模态模型（MLM）**、**视觉语言模型（VLM）**、**文本嵌入模型（Embedding）** 和**重排序模型（Reranker）**。所有模型文件均需存储于设备内置的 **CFexpress Type-B** 存储卡 中，用户需使用兼容的 **CFexpress Type-B** 读卡器 在主机端进行模型的上传、管理与更新。

当 **CFexpress Type-B** 存储卡接入 RM-01 后，系统会将其挂载为名为 `models` 的只读数据卷，路径为 `/home/rm01/models`。其标准文件结构如下：

```bash
models/
├── auto/                  # 系统自动加载模型目录（生产级部署）
│   ├── embedding/         # 嵌入模型（系统不自动加载，见下文说明）
│   ├── llm/               # 大语言模型（权重文件直接存放，见下文）
│   └── reranker/          # 重排序模型（系统不自动加载，见下文）
└── dev/                   # 开发者自定义模型目录（高优先级）
    ├── embedding/
    ├── llm/
    └── reranker/
```

> **说明**：  
> - `auto/` 目录用于**轻量级、标准化模型部署**，由系统自动识别；  
> - `dev/` 目录用于**开发者精细控制模型行为**，优先级高于 `auto/`，系统将忽略 `auto/` 中的模型。

---

### 1. 自动模式（auto）

#### 使用方式：
将模型的**完整权重文件**（如 `.safetensors`、`.bin`、`.pt`、`.awq` 等）**直接放置于 `auto/llm/` 目录下**，**禁止使用子文件夹嵌套**。

> 错误示例：  
> `auto/llm/Qwen3-30B-A3B-Instruct-2507-AWQ/model.safetensors`  
>  
> 正确示例：  
> `auto/llm/model-001-of-006.safetensors`  
> `auto/llm/config.json`
> `auto/llm/tokenizer.json`
> `auto/llm/vocab.json`

#### 系统行为：
- 设备开机后，系统将扫描 `auto/llm/` 目录，自动加载符合兼容格式的模型；
- **不支持嵌入模型（embedding）与重排序模型（reranker）的自动加载**，仅支持 LLM；
- 模型加载后，**默认启用基础推理能力**，**不开启以下高级功能**：
  - Speculative Decoding（推测解码）
  - Prefix Caching（前缀缓存）
  - Chunked Prefill（分块预填充）
  - 最大上下文长度（`max_model_len`）将被限制为系统安全阈值（通常 ≤ 8192 tokens）；
- **性能优化受限**：为保障系统稳定性与多任务并发能力，自动模式下的模型将使用保守的显存分配策略（`gpu_memory_utilization` ≤ 0.8）。

>  **重要提示**：  
> 自动模式适用于**快速验证模型兼容性**或**标准化部署场景**，**不适用于生产级高性能推理**。如需完整性能，请使用**手动模式（dev）**。

---

### 2. 手动模式（dev）—— 开发者模式

> **手动模式优先级高于自动模式**。当 `dev/` 的`embedding` `llm` `reranker` 三个目录下存在任意 `.yaml` 配置文件时，系统将**完全忽略 `auto/` 中的所有模型**。

#### 配置文件结构：
在 `dev/` 的`embedding` `llm` `reranker` 三个目录下，需分别放置以下三个 YAML 配置文件，分别用于启动对应模型服务：

| 文件名 | 用途 |
|--------|------|
| `embedding_run.yaml` | 启动嵌入模型服务（Text Embedding） |
| `llm_run.yaml` | 启动大语言模型服务（LLM） |
| `reranker_run.yaml` | 启动重排序模型服务（Reranker） |

> 所有文件必须位于 `dev/` 目录下相对应的文件夹内，**文件名必须完全匹配**，大小写敏感。

#### 示例配置文件（`llm_run.yaml`）：

```yaml
model: /home/rm01/models/dev/llm/Qwen3-30B-A3B-Instruct-2507-AWQ
port: 58000
gpu_memory_utilization: 0.85
max_model_len: 24576
served_model_name: RM-01 LLM
enable_prefix_caching: true
enable_chunked_prefill: true
max_num_batched_tokens: 512
block_size: 16
tensor_parallel_size: 1
dtype: auto
```

> **说明**：
> - `model` 路径必须为**绝对路径**，指向模型文件所在目录（非压缩包）；
> - `port` 必须位于 `58000–58999` 范围内，且不可与其他服务冲突；
> - `gpu_memory_utilization` 建议设置为 0.7–0.9，以最大化吞吐量；
> - 所有参数均遵循 **vLLM 官方文档规范** `https://docs.vllm.ai/en/latest/index.html`，请确保配置项兼容 **CFexpress Type-B** 存储卡内目前使用的 vLLM 版本。

#### 启动流程：
1. 将模型文件（完整目录）复制至 `dev/llm/`（或 `embedding/`、`reranker/`）；
2. 编写并放置对应的 `.yaml` 配置文件；
3. 插入 **CFexpress Type-B** 存储卡并重启 RM-01；
4. 系统将自动加载 `dev/` 中的配置，并启动对应服务；
5. 服务启动后，可通过 `http://10.10.99.98:58000/v1/chat/completions` 等接口访问。

> **建议**：首次部署时，建议使用 `vLLM` 官方提供的 `--load-format auto` 和 `--dtype auto`，以自动适配模型格式。

---

### 安全与维护须知

- **禁止直接 SSH 登录推理模组**：所有模型管理必须通过 `CFexpress Type-B` 存储卡完成；
- **模型文件必须为原始权重**：禁止使用压缩包（.zip/.tar.gz）、加密包或非标准格式；
- **文件权限**：所有模型文件需为可读（`chmod 644`），目录需可执行（`chmod 755`）；
- **版本控制**：建议使用 Git 或文件命名规范（如 `Qwen3-30B-A3B-Instruct-v1.2-20250930`）管理模型版本；
- **备份建议**：每次更新模型前，请备份 `dev/` 和 `auto/` 目录，避免配置丢失。

---

### 模式选择建议

| 场景 | 推荐模式 |
|------|----------|
| 快速验证模型是否兼容 | 自动模式（auto） |
| 生产环境高性能推理 | 手动模式（dev） + 精细配置 |
| 多模型并行部署 | 手动模式（dev） + 多个 `.yaml` 文件 |
| 开发调试、原型验证 | 手动模式（dev） |

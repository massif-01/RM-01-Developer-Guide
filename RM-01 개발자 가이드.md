# RM-01 개발자 가이드

**사용 전 필독:**

RM-01은 **추론 모듈**, **애플리케이션 모듈**, 그리고 **암호화 및 관리 칩**(이하 관리 모듈)으로 구성되어 있으며, 이들은 **온보드 이더넷 스위치 칩**을 통해 상호 연결되어 내부 LAN 서브넷을 형성합니다. 사용자가 **USB Type-C** 인터페이스를 통해 호스트(예: PC, 스마트폰, iPad)에 연결하면, RM-01은 USB 이더넷 기능을 통해 호스트에 가상 이더넷 인터페이스를 제공합니다. 그러면 호스트는 IP 주소를 획득하고 자동으로 서브넷에 참여하여 데이터를 교환합니다.

장치가 전원을 켜고 **USB Type-C** 인터페이스를 통해 호스트에 연결된 후, 시스템은 로컬 네트워크 서브넷을 자동으로 구성합니다. **사용자 호스트**는 정적 IP 주소 `10.10.99.100`을 할당받습니다. **추론 모듈**(IP: `10.10.99.98`)과 **애플리케이션 모듈**(IP: `10.10.99.99`)은 각각 독립적인 SSH 서비스를 배포하며, 사용자는 표준 SSH 클라이언트(예: OpenSSH, PuTTY)를 통해 직접 접근할 수 있습니다. 반면, 관리 모듈은 직렬 포트 도구를 통해 접근해야 합니다.

---

## 호스트에서 RM-01에 인터넷 액세스를 제공하는 방법 (macOS 예시)

**사용자 호스트**를 USB Type-C로 연결한 후, RM-01은 네트워크 인터페이스 목록에 다음과 같이 표시됩니다:  
- **`AX88179A`** (개발자 버전)  
- **`RMinte RM-01`** (상용 릴리스 버전)

인터넷 공유를 설정하려면 다음 단계를 따르세요:

1. **시스템 설정** (System Settings)을 엽니다  
2. **네트워크** (Network) → **공유** (Sharing)로 이동합니다  
3. **인터넷 공유** (Internet Sharing)를 활성화합니다  
4. 공유 설정 옆의 **“i” 아이콘**을 클릭하여 설정 인터페이스에 들어갑니다:  
   - **“다음에서 연결 공유”**를 **Wi-Fi**로 설정합니다  
   - **“다음 포트를 사용하여 컴퓨터에 공유”**에서 **AX88179A** 또는 **RMinte RM-01** (장치 모델에 따라 선택)을 체크합니다  
5. **완료** (Done)를 클릭합니다

그런 다음, **네트워크** (Network) 설정 페이지로 돌아가 RM-01의 네트워크 인터페이스를 수동으로 구성합니다:  
- **IP 주소**: `10.10.99.100`  
- **서브넷 마스크**: `255.255.255.0`  
- **라우터** (Router): `10.10.99.100` (즉, 호스트 자체의 IP)

> **참고**:  
> 이 구성은 호스트를 게이트웨이로 설정하여 RM-01에 NAT 네트워크 액세스를 제공합니다. RM-01의 기본 게이트웨이와 DNS는 호스트의 DHCP 서비스를 통해 자동으로 할당됩니다. IP를 수동으로 설정하면 장치가 항상 `10.10.99.0/24` 서브넷 내에 유지되어 장치 내부 서비스 통신과 일관성을 유지합니다.

---

## **CFexpress Type-B** 저장 카드 정보

**CFexpress Type-B** 저장 카드는 RM-01 장치의 핵심 구성 요소 중 하나로, 시스템 부팅, 추론 프레임워크 배포, ISV/SV 소프트웨어 배포 및 인증 인증과 같은 주요 기능을 담당합니다.

저장 카드는 다음과 같은 세 개의 독립적인 파티션으로 나뉩니다:

`rm01sys` `rm01app` `rm01models`

`rm01sys` — 시스템 파티션  
**추론 모듈**의 운영 체제와 핵심 실행 환경이 이 파티션에 설치되어 있습니다. 사용자 또는 개발자가 이 파티션의 내용에 접근, 수정 또는 삭제하는 것은 엄격히 금지됩니다. 승인되지 않은 변경은 추론 모듈이 부팅되지 않거나 추론 기능이 작동하지 않게 할 수 있으며, 이로 인한 하드웨어 또는 소프트웨어 손상은 어떠한 보증 서비스 범위에 포함되지 않습니다.

`rm01app` — 애플리케이션 파티션  
이 파티션은 사용자 또는 개발자가 제출한 `Docker` 이미지 파일을 일시적으로 저장하는 데 사용됩니다. 이미지가 `rm01app`에 기록된 후, RM-01 시스템은 이를 호스트에 내장된 **NVMe SSD** 저장소로 자동 마이그레이션하고 컨테이너화된 배포를 완료합니다. 이 파티션에서 애플리케이션 파일을 직접 실행하거나 수정하지 마세요.

`rm01models` — 모델 파티션  
이 파티션은 사용자 또는 개발자가 로드하는 대규모 AI 모델(예: LLM, 멀티모달 모델 등)을 저장하는 데 전용입니다. 모델 형식, 크기 제한, 로드 절차 및 호환성 요구 사항에 대한 자세한 내용은 아래 “모델 정보” 섹션을 참조하세요.

---

## 애플리케이션 모듈 정보

IP 주소: `10.10.99.99`  
포트 범위: `59000-59299`

애플리케이션 모듈 하드웨어 사양:

```
프로세서: Intel Core i3-N305 (8코어 8스레드, 기본 주파수 1.8 GHz, 최대 터보 주파수 3.8 GHz)  
메모리: 16 GB / 24 GB LPDDR5-4800MT/s (온보드, 확장 불가)  
저장소: 512 GB / 1 TB / 2 TB (옵션) NVMe SSD  
```

애플리케이션 모듈 SSH 액세스 자격 증명:

```
기본 사용자 이름: rm01  
기본 비밀번호: rm01 (공장 설정, 최초 로그인 전용)  
```

**보안 주의사항:**  
시스템 보안을 보장하기 위해, 최초 SSH 로그인 후 즉시 `passwd` 명령을 사용하여 기본 비밀번호를 변경하세요. 기본 비밀번호는 초기 구성용으로만 사용되며, 프로덕션 및 배포 환경에서는 절대 사용해서는 안 됩니다.

---

## 추론 모듈 정보

IP 주소: `10.10.99.98`  
서비스 포트 범위: `58000–58999`

**추론 모듈**은 RM-01의 핵심 컴퓨팅 유닛으로, 다양한 고성능 AI 추론 구성을 지원합니다. 사용자는 모델 규모와 성능 요구 사항에 따라 적절한 모델을 선택할 수 있습니다. 공장 출고 시 사전 설정된 네 가지 하드웨어 구성은 다음과 같습니다:

| 메모리 | 메모리 대역폭 | 연산 능력          | Tensor Core 수 |
|--------|---------------|:------------------|:---------------|
| 32 GB  | 204.8 GB/s    | 200 TOPS (INT8)   | 56             |
| 64 GB  | 204.8 GB/s    | 275 TOPS (INT8)   | 64             |
| 64 GB  | 273 GB/s      | 1,200 TFLOPS (FP4)| 64             |
| 128 GB | 273 GB/s      | 2,070 TFLOPS (FP4)| 96             |

RM-01은 **CFexpress Type-B** 저장 카드에 다음 두 가지 추론 프레임워크가 사전 설치되어 있으며, 둘 다 추론 모듈에서 실행됩니다:

`vLLM`: 자동으로 시작되며, 기본적으로 포트 **58000**을 수신하고, OpenAI 호환 API 인터페이스를 제공하며, POST /v1/chat/completions와 같은 표준 요청을 지원합니다.  
`TEI` (Text Embedding Inference): 수동으로 시작해야 하며, 텍스트 임베딩 서비스에 사용됩니다.

API 액세스 방법:  
모델이 성공적으로 로드된 후, **vLLM** 추론 서비스는 다음 주소를 통해 액세스할 수 있습니다:  
`http://10.10.99.98:58000/v1/chat/completions`  
표준 OpenAI 클라이언트(예: openai-python, curl, Postman)를 사용하여 직접 호출할 수 있습니다.

**보안 주의사항:**  
시스템 보안과 안정성을 보장하기 위해, 추론 모듈은 SSH 액세스 권한을 제공하지 않습니다. 사용자와 개발자는 이 모듈의 기본 운영 체제에 직접 로그인하거나 대화형으로 조작할 수 없습니다. 보안 정책을 우회하거나 추론 모듈에 직접 접근하려는 시도는 시스템 이상, 데이터 손상 또는 서비스 중단을 초래할 수 있으며, 이는 보증 서비스 범위에 포함되지 않습니다.

---

## 모델 정보

RM-01은 다양한 AI 모델의 추론을 지원하며, 이에 국한되지 않습니다: **대규모 언어 모델(LLM)**, **멀티모달 모델(MLM)**, **비전-언어 모델(VLM)**, **텍스트 임베딩 모델(Embedding)**, **리랭커 모델(Reranker)**. 모든 모델 파일은 장치에 내장된 **CFexpress Type-B** 저장 카드에 저장되어야 하며, 사용자는 호환 가능한 **CFexpress Type-B** 카드 리더를 사용하여 호스트 측에서 모델을 업로드, 관리 및 업데이트해야 합니다.

**CFexpress Type-B** 저장 카드가 RM-01에 연결되면, 시스템은 이를 `models`라는 이름의 읽기 전용 데이터 볼륨으로 마운트하며, 경로는 `/home/rm01/models`입니다. 표준 파일 구조는 다음과 같습니다:

```bash
models/
├── auto/                  # 자동 모델 로드 디렉토리 (프로덕션급 배포)
│   ├── embedding/         # 임베딩 모델 (자동 로드 불가, 아래 참조)
│   ├── llm/               # 대규모 언어 모델 (웨이트 파일 직접 저장, 아래 참조)
│   └── reranker/          # 리랭커 모델 (자동 로드 불가, 아래 참조)
└── dev/                   # 개발자 정의 모델 디렉토리 (우선순위 높음)
    ├── embedding/
    ├── llm/
    └── reranker/
```

> **참고**:  
> - `auto/` 디렉토리는 **경량화 및 표준화된 모델 배포**에 사용되며, 시스템에서 자동으로 인식됩니다.  
> - `dev/` 디렉토리는 **개발자의 세밀한 모델 동작 제어**에 사용되며, `auto/`보다 우선순위가 높아 시스템은 `auto/` 내의 모델을 무시합니다.

---

### 1. 자동 모드 (auto)

#### 사용 방법:  
모델의 **완전한 웨이트 파일** (예: `.safetensors`, `.bin`, `.pt`, `.awq` 등)을 **`auto/llm/` 디렉토리에 직접 배치**하며, **하위 폴더에 중첩시키지 마세요**.

> 잘못된 예:  
> `auto/llm/Qwen3-30B-A3B-Instruct-2507-AWQ/model.safetensors`  
>  
> 올바른 예:  
> `auto/llm/model-001-of-006.safetensors`  
> `auto/llm/config.json`  
> `auto/llm/tokenizer.json`  
> `auto/llm/vocab.json`

#### 시스템 동작:  
- 장치 부팅 시, 시스템은 `auto/llm/` 디렉토리를 스캔하여 호환 가능한 형식의 모델을 자동으로 로드합니다.  
- **임베딩 모델 또는 리랭커 모델의 자동 로드는 지원되지 않으며**, LLM만 지원됩니다.  
- 모델 로드 후, **기본적으로 기본 추론 기능이 활성화**되며, 다음과 같은 **고급 기능은 활성화되지 않습니다**:  
  - 투기적 디코딩 (Speculative Decoding)  
  - 프리픽스 캐싱 (Prefix Caching)  
  - 청크드 프리필 (Chunked Prefill)  
  - 최대 컨텍스트 길이 (`max_model_len`)는 시스템의 안전 임계값(일반적으로 ≤ 8192 토큰)으로 제한됩니다.  
- **성능 최적화 제한**: 시스템 안정성과 멀티태스킹 동시성을 보장하기 위해, 자동 모드의 모델은 보수적인 메모리 할당 전략(`gpu_memory_utilization` ≤ 0.8)을 사용합니다.

> **중요 참고**:  
> 자동 모드는 **모델 호환성을 빠르게 확인**하거나 **표준화된 배포 시나리오**에 적합하지만, **프로덕션 환경에서의 고성능 추론에는 적합하지 않습니다**. 전체 성능을 위해 **수동 모드 (dev)**를 사용하세요.

---

### 2. 수동 모드 (dev) — 개발자 모드

> **수동 모드는 자동 모드보다 우선순위가 높습니다**. `dev/`의 `embedding`, `llm`, 또는 `reranker` 디렉토리에 `.yaml` 설정 파일이 존재하면, 시스템은 **`auto/` 내의 모든 모델을 완전히 무시**합니다.

#### 설정 파일 구조:  
`dev/`의 `embedding`, `llm`, 및 `reranker` 디렉토리에는 해당 모델 서비스를 시작하기 위해 다음 세 개의 YAML 설정 파일이 있어야 합니다:

| 파일 이름            | 용도                              |
|----------------------|-----------------------------------|
| `embedding_run.yaml` | 텍스트 임베딩 모델 서비스 시작    |
| `llm_run.yaml`       | 대규모 언어 모델 서비스 시작      |
| `reranker_run.yaml`  | 리랭커 모델 서비스 시작           |

> 모든 파일은 `dev/` 내의 해당 폴더에 위치해야 하며, **파일 이름은 정확히 일치**해야 하며, 대소문자를 구분합니다.

#### 설정 파일 예시 (`llm_run.yaml`):

```yaml
model: /home/rm01/models/dev/llm/Qwen3-30B-A3B-Instruct-2507-AWQ
port: 58000
gpu_memory_utilization: 0.85
max_model_len: 24576
served_model_name: RM-01 LLM
enable_prefix_caching: true
enable_chunked_prefill: true
max_num_batched_tokens: 512
block_size: 16
tensor_parallel_size: 1
dtype: auto
```

> **참고**:  
> - `model` 경로는 **절대 경로**로, 모델 파일 디렉토리(압축 파일 아님)를 가리켜야 합니다.  
> - `port`는 `58000–58999` 범위 내에 있어야 하며, 다른 서비스와 충돌하지 않아야 합니다.  
> - `gpu_memory_utilization`은 처리량을 최대화하기 위해 0.7–0.9로 설정하는 것이 좋습니다.  
> - 모든 매개변수는 **vLLM 공식 문서** `https://docs.vllm.ai/en/latest/index.html`를 따릅니다. **CFexpress Type-B** 저장 카드에서 현재 사용 중인 vLLM 버전과 호환되는지 확인하세요.

#### 시작 프로세스:  
1. 모델 파일(전체 디렉토리)을 `dev/llm/` (또는 `embedding/`, `reranker/`)에 복사합니다.  
2. 해당 `.yaml` 설정 파일을 작성하여 배치합니다.  
3. **CFexpress Type-B** 저장 카드를 삽입하고 RM-01을 재부팅합니다.  
4. 시스템은 `dev/` 내의 설정을 자동으로 로드하여 해당 서비스를 시작합니다.  
5. 서비스 시작 후, `http://10.10.99.98:58000/v1/chat/completions`와 같은 인터페이스를 통해 액세스할 수 있습니다.

> **권장**: 첫 배포 시, vLLM이 제공하는 `--load-format auto` 및 `--dtype auto` 옵션을 사용하여 모델 형식에 자동으로 적응하는 것이 좋습니다.

---

### 보안 및 유지 관리 주의사항

- **추론 모듈에 직접 SSH 로그인 금지**: 모든 모델 관리는 **CFexpress Type-B** 저장 카드를 통해 이루어져야 합니다.  
- **모델 파일은 원시 웨이트여야 함**: 압축 파일(.zip/.tar.gz), 암호화 패키지 또는 비표준 형식을 사용하지 마세요.  
- **파일 권한**: 모든 모델 파일은 읽기 가능(`chmod 644`)이어야 하며, 디렉토리는 실행 가능(`chmod 755`)이어야 합니다.  
- **버전 관리**: Git 또는 파일 명명 규칙(예: `Qwen3-30B-A3B-Instruct-v1.2-20250930`)을 사용하여 모델 버전을 관리하는 것이 좋습니다.  
- **백업 권장**: 모델 업데이트 전에 `dev/` 및 `auto/` 디렉토리를 백업하여 설정 손실을 방지하세요.

---

### 모드 선택 권장사항

| 시나리오                        | 권장 모드                     |
|--------------------------------|-------------------------------|
| 모델 호환성 빠른 확인           | 자동 모드 (auto)              |
| 프로덕션 환경에서의 고성능 추론 | 수동 모드 (dev) + 세밀한 설정 |
| 다중 모델 병렬 배포            | 수동 모드 (dev) + 다중 `.yaml` 파일 |
| 개발 디버깅, 프로토타입 검증    | 수동 모드 (dev)               |